# Monitoring Vertex AI with Gemini LLMs

Companion materials

## Abstract

Join Datadog and Google Cloud for a webinar on LLM observability with a focus on monitoring Vertex AI with Gemini. You’ll learn how to get the most out of your large language models, as well as Datadog’s pragmatic approach to monitoring and observability.

We’ll discuss generative AI application monitoring topics including:
- Hallucinations and evaluations
- Cost, speed, and quality trade-offs
- LLM application chains and end-to-end observability
- Trust and privacy concerns

In this session, you'll learn about generative AI deployments as well as how Datadog monitors the health and performance of your Vertex AI with Gemini LLM footprint!

## Demo

- [GCP Demo deployment](https://github.com/GoogleCloudPlatform/microservices-demo)

## Links

### Docs

- [Datadog LLM Observability instrumentation documentation](https://docs.datadoghq.com/llm_observability/setup/auto_instrumentation/)
- [Innovate faster with enterprise-ready AI, enhanced by Gemini models](https://cloud.google.com/vertex-ai?hl=en#build-with-gemini)

### Blogs

- [Monitor your Google Gemini apps with Datadog LLM Observability](https://www.datadoghq.com/blog/monitor-google-gemini-datadog-llm-observability/)
- [Best practices for monitoring LLM prompt injection attacks to protect sensitive data](https://www.datadoghq.com/blog/monitor-llm-prompt-injection-attacks/)
- [Get granular LLM observability by instrumenting your LLM chains](https://www.datadoghq.com/blog/llm-observability-chain-tracing/)
